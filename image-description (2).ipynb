{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. INSTALL & IMPORT\n\n!pip install transformers accelerate timm datasets ultralytics nltk -q\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nimport json\nfrom ultralytics import YOLO\n\nnltk.download('punkt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:25:17.389287Z","iopub.execute_input":"2025-09-17T10:25:17.389551Z","iopub.status.idle":"2025-09-17T10:27:17.306858Z","shell.execute_reply.started":"2025-09-17T10:25:17.389524Z","shell.execute_reply":"2025-09-17T10:27:17.305963Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-09-17 10:26:55.870373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758104816.254885      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758104816.359776      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# 2. DEVICE CONFIG\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:27:45.559461Z","iopub.execute_input":"2025-09-17T10:27:45.560218Z","iopub.status.idle":"2025-09-17T10:27:45.565341Z","shell.execute_reply.started":"2025-09-17T10:27:45.560186Z","shell.execute_reply":"2025-09-17T10:27:45.564609Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 3. PATHS\n\nIMAGES_PATH = \"/kaggle/input/flickr30k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/flickr30k/captions.txt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:27:50.520717Z","iopub.execute_input":"2025-09-17T10:27:50.521503Z","iopub.status.idle":"2025-09-17T10:27:50.524912Z","shell.execute_reply.started":"2025-09-17T10:27:50.521480Z","shell.execute_reply":"2025-09-17T10:27:50.524079Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 4. LOAD CAPTIONS\n\ndf = pd.read_csv(CAPTIONS_FILE)\ndf.columns = ['image', 'caption']\ndf = df.dropna()\n\nprint(\"Sample data:\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:27:54.756558Z","iopub.execute_input":"2025-09-17T10:27:54.757277Z","iopub.status.idle":"2025-09-17T10:27:55.249385Z","shell.execute_reply.started":"2025-09-17T10:27:54.757251Z","shell.execute_reply":"2025-09-17T10:27:55.248786Z"}},"outputs":[{"name":"stdout","text":"Sample data:\n            image                                            caption\n0  1000092795.jpg   Two young guys with shaggy hair look at their...\n1  1000092795.jpg   Two young , White males are outside near many...\n2  1000092795.jpg   Two men in green shirts are standing in a yard .\n3  1000092795.jpg       A man in a blue shirt standing in a garden .\n4  1000092795.jpg            Two friends enjoy time spent together .\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 5. TRAIN / VAL / TEST SPLIT\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\ntest_df = test_df.sample(n=100, random_state=42)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:28:01.121611Z","iopub.execute_input":"2025-09-17T10:28:01.122214Z","iopub.status.idle":"2025-09-17T10:28:01.149791Z","shell.execute_reply.started":"2025-09-17T10:28:01.122188Z","shell.execute_reply":"2025-09-17T10:28:01.149084Z"}},"outputs":[{"name":"stdout","text":"Train: 127131, Val: 15891, Test: 100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":" #6. YOLO DETECTION (PRECOMPUTE)\n\nyolo_model = YOLO(\"yolov8s.pt\")  # lightweight model\n\ndef detect_objects(image_path):\n    results = yolo_model(image_path, verbose=False)\n    detections = []\n    for r in results:\n        for box in r.boxes:\n            cls_id = int(box.cls[0])\n            label = yolo_model.names[cls_id]\n            bbox = box.xyxy[0].cpu().numpy().tolist()  # [x1,y1,x2,y2]\n            detections.append({\"label\": label, \"bbox\": bbox})\n    return detections\n\n# Cache YOLO detections for train/val/test\nyolo_detections = {}\nfor img_name in tqdm(df['image'].unique(), desc=\"Running YOLO on dataset\"):\n    img_path = os.path.join(IMAGES_PATH, img_name)\n    try:\n        yolo_detections[img_name] = detect_objects(img_path)\n    except:\n        yolo_detections[img_name] = []\nwith open(\"yolo_detections.json\", \"w\") as f:\n    json.dump(yolo_detections, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:28:07.983589Z","iopub.execute_input":"2025-09-17T10:28:07.983903Z","iopub.status.idle":"2025-09-17T10:48:20.155031Z","shell.execute_reply.started":"2025-09-17T10:28:07.983883Z","shell.execute_reply":"2025-09-17T10:48:20.154361Z"}},"outputs":[{"name":"stdout","text":"\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ━━━━━━━━━━━━ 21.5MB 153.0MB/s 0.1s0.1s<0.1s\n","output_type":"stream"},{"name":"stderr","text":"Running YOLO on dataset: 100%|██████████| 31783/31783 [20:09<00:00, 26.27it/s]  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 7. DATASET WITH CROPS\n\nclass Flickr30kYOLOCropsDataset(Dataset):\n    def __init__(self, dataframe, image_path, processor, detections_dict):\n        self.dataframe = dataframe\n        self.image_path = image_path\n        self.processor = processor\n        self.detections_dict = detections_dict\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_file = os.path.join(self.image_path, row['image'])\n        image = Image.open(image_file).convert('RGB')\n        caption = row['caption']\n\n        detections = self.detections_dict.get(row['image'], [])\n        labels = [d[\"label\"] for d in detections]\n        context_text = \"Objects detected: \" + \", \".join(labels)\n\n        # Crops\n        object_images = []\n        for d in detections[:3]:  # take up to 3 detections\n            x1, y1, x2, y2 = d[\"bbox\"]\n            crop = image.crop((x1, y1, x2, y2))\n            object_images.append(crop)\n        \n        # pad with original image until we have exactly 3 crops\n        while len(object_images) < 3:\n            object_images.append(image)\n        \n        # now always 4 images: 1 original + 3 crops\n        inputs = self.processor(\n            images=[image] + object_images,\n            text=context_text + \" | \" + caption,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=50,\n            truncation=True\n        )\n\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"],  # shape: (N_images, C, H, W)\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze()\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:23:42.995177Z","iopub.execute_input":"2025-09-17T11:23:42.996194Z","iopub.status.idle":"2025-09-17T11:23:43.009204Z","shell.execute_reply.started":"2025-09-17T11:23:42.996140Z","shell.execute_reply":"2025-09-17T11:23:43.008371Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# 8. INIT BLIP\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:48:34.744630Z","iopub.execute_input":"2025-09-17T10:48:34.745400Z","iopub.status.idle":"2025-09-17T10:48:42.142541Z","shell.execute_reply.started":"2025-09-17T10:48:34.745361Z","shell.execute_reply":"2025-09-17T10:48:42.141279Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b697fc0d7e4cd3972b6c0cffe41a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ed70553e164f36b390126094a35338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e92c05c58184e318528e6a62119391e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f06dcd6a4b4c24bde703fd3fb162d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d65e35652e4dad82886e6c5e8e1351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffd81787e5447deb65b6f9c08209eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b44464980b541c89687d6c7f07de03f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae28d7c67da4dad98e52682257c1911"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 9. DATALOADERS \ntrain_dataset = Flickr30kYOLOCropsDataset(train_df, IMAGES_PATH, processor, yolo_detections)\nval_dataset = Flickr30kYOLOCropsDataset(val_df, IMAGES_PATH, processor, yolo_detections)\ntest_dataset = Flickr30kYOLOCropsDataset(test_df, IMAGES_PATH, processor, yolo_detections) \n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True) \nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:23:49.905102Z","iopub.execute_input":"2025-09-17T11:23:49.905940Z","iopub.status.idle":"2025-09-17T11:23:49.913446Z","shell.execute_reply.started":"2025-09-17T11:23:49.905903Z","shell.execute_reply":"2025-09-17T11:23:49.912550Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 10. TRAINING\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nscaler = torch.amp.GradScaler(\"cuda\")\n\nepochs = 2\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        pixel_values = batch[\"pixel_values\"].to(DEVICE)   # (B, N, 3, H, W)\n        input_ids = batch[\"input_ids\"].to(DEVICE)         # (B, seq_len)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n        # ---- FIX: flatten crops into batch dimension ----\n        B, N, C, H, W = pixel_values.shape\n        pixel_values = pixel_values.view(B * N, C, H, W)\n        input_ids = input_ids.repeat_interleave(N, dim=0)\n        attention_mask = attention_mask.repeat_interleave(N, dim=0)\n        # -------------------------------------------------\n\n        with torch.autocast(\"cuda\", enabled=True):\n            outputs = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=input_ids\n            )\n            loss = outputs.loss\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n        loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n        loop.set_postfix(loss=loss.item())\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n\n    # ---- Validation ----\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            # flatten crops in validation too\n            B, N, C, H, W = pixel_values.shape\n            pixel_values = pixel_values.view(B * N, C, H, W)\n            input_ids = input_ids.repeat_interleave(N, dim=0)\n            attention_mask = attention_mask.repeat_interleave(N, dim=0)\n\n            with torch.autocast(\"cuda\", enabled=True):\n                outputs = model(\n                    pixel_values=pixel_values,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                val_loss += outputs.loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:26:31.489311Z","iopub.execute_input":"2025-09-17T11:26:31.490284Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/2]:  32%|███▏      | 10142/31783 [2:51:12<6:03:53,  1.01s/it, loss=0.556]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 11. SAVE MODEL\n\nmodel.save_pretrained(\"/kaggle/working/blip-yolo-flickr30k\")\nprocessor.save_pretrained(\"/kaggle/working/blip-yolo-flickr30k\")\nprint(\"Model saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 12. TESTING + BLEU\n\nmodel.eval()\nbleu_scores = []\n\nfor idx in range(len(test_dataset)):\n    data = test_dataset[idx]\n    pixel_values = data[\"pixel_values\"].unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        generated_ids = model.generate(pixel_values=pixel_values, max_length=40)\n        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n\n    reference = [test_df.iloc[idx]['caption'].split()]\n    candidate = caption.split()\n    bleu = sentence_bleu(reference, candidate)\n    bleu_scores.append(bleu)\n\nprint(f\"Average BLEU Score on Test Set: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}