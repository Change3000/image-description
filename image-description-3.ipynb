{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate timm datasets nltk -q\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:10:17.243790Z","iopub.execute_input":"2025-09-05T23:10:17.244066Z","iopub.status.idle":"2025-09-05T23:10:20.748939Z","shell.execute_reply.started":"2025-09-05T23:10:17.244043Z","shell.execute_reply":"2025-09-05T23:10:20.747941Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# 2. DEVICE CONFIG\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:10:37.941305Z","iopub.execute_input":"2025-09-05T23:10:37.941630Z","iopub.status.idle":"2025-09-05T23:10:37.946962Z","shell.execute_reply.started":"2025-09-05T23:10:37.941604Z","shell.execute_reply":"2025-09-05T23:10:37.946251Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 3. PATHS (HARDCODED)\n\nIMAGES_PATH = \"/kaggle/input/flickr30k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/flickr30k/captions.txt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:10:52.268376Z","iopub.execute_input":"2025-09-05T23:10:52.268666Z","iopub.status.idle":"2025-09-05T23:10:52.272688Z","shell.execute_reply.started":"2025-09-05T23:10:52.268649Z","shell.execute_reply":"2025-09-05T23:10:52.272001Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 4. LOAD DATASET\n\ndf = pd.read_csv(CAPTIONS_FILE)\ndf.columns = ['image', 'caption']  # only 2 columns\n\n# Remove any NaN values\ndf = df.dropna()\n\nprint(\"Sample data:\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:11:20.646205Z","iopub.execute_input":"2025-09-05T23:11:20.646904Z","iopub.status.idle":"2025-09-05T23:11:21.135854Z","shell.execute_reply.started":"2025-09-05T23:11:20.646879Z","shell.execute_reply":"2025-09-05T23:11:21.135111Z"}},"outputs":[{"name":"stdout","text":"Sample data:\n            image                                            caption\n0  1000092795.jpg   Two young guys with shaggy hair look at their...\n1  1000092795.jpg   Two young , White males are outside near many...\n2  1000092795.jpg   Two men in green shirts are standing in a yard .\n3  1000092795.jpg       A man in a blue shirt standing in a garden .\n4  1000092795.jpg            Two friends enjoy time spent together .\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 5. SPLIT INTO TRAIN, VAL, TEST\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Reducing test size for quick evaluation, earlier it was taking too lng\ntest_df = test_df.sample(n=100, random_state=42)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:12:20.533306Z","iopub.execute_input":"2025-09-05T23:12:20.533815Z","iopub.status.idle":"2025-09-05T23:12:20.562394Z","shell.execute_reply.started":"2025-09-05T23:12:20.533789Z","shell.execute_reply":"2025-09-05T23:12:20.561767Z"}},"outputs":[{"name":"stdout","text":"Train: 127131, Val: 15891, Test: 100\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 6. DATASET CLASS\n\nclass Flickr30kDataset(Dataset):\n    def __init__(self, dataframe, image_path, processor):\n        self.dataframe = dataframe\n        self.image_path = image_path\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_file = os.path.join(self.image_path, row['image'])\n        image = Image.open(image_file).convert('RGB')\n        caption = row['caption']\n\n        inputs = self.processor(\n            images=image,\n            text=caption,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=30,\n            truncation=True\n        )\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"].squeeze(),\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:13:12.584942Z","iopub.execute_input":"2025-09-05T23:13:12.585572Z","iopub.status.idle":"2025-09-05T23:13:12.591470Z","shell.execute_reply.started":"2025-09-05T23:13:12.585546Z","shell.execute_reply":"2025-09-05T23:13:12.590667Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 7. INIT PROCESSOR & MODEL\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = model.to(DEVICE)\n\n\n# 8. CREATE DATALOADERS\n\ntrain_dataset = Flickr30kDataset(train_df, IMAGES_PATH, processor)\nval_dataset = Flickr30kDataset(val_df, IMAGES_PATH, processor)\ntest_dataset = Flickr30kDataset(test_df, IMAGES_PATH, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:13:50.932571Z","iopub.execute_input":"2025-09-05T23:13:50.933284Z","iopub.status.idle":"2025-09-05T23:13:58.415968Z","shell.execute_reply.started":"2025-09-05T23:13:50.933256Z","shell.execute_reply":"2025-09-05T23:13:58.415027Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa486fbab3e4aeaac2a33f591448b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17116e3ef29452ba1ed55e637978086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84bad14a45b4ef28bac54dc47fc7957"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e22b69132c40868e15f6a815b3644d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f935ea6d7b4499e844ab058975bc1b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282afe72a35d4c59ae04bd8df1d13633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7aeddc14694f3abfb39a6ac23b8e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920e70d10cac4b95a05ae9182e347841"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 9. TRAINING SETUP\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nscaler = torch.amp.GradScaler(\"cuda\")\n\nepochs = 3 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:15:03.856656Z","iopub.execute_input":"2025-09-05T23:15:03.857287Z","iopub.status.idle":"2025-09-05T23:15:03.863421Z","shell.execute_reply.started":"2025-09-05T23:15:03.857260Z","shell.execute_reply":"2025-09-05T23:15:03.862796Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 10. TRAINING LOOP\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n        with torch.autocast(\"cuda\", enabled=(DEVICE==\"cuda\")):\n            outputs = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=input_ids\n            )\n            loss = outputs.loss\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n        loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n        loop.set_postfix(loss=loss.item())\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            with torch.autocast(\"cuda\", enabled=(DEVICE==\"cuda\")):\n                outputs = model(\n                    pixel_values=pixel_values,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                val_loss += outputs.loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:15:53.776911Z","iopub.execute_input":"2025-09-05T23:15:53.777437Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/3]:   1%|â–         | 202/15892 [01:43<2:15:26,  1.93it/s, loss=1.49] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 11. SAVING MODEL\n\nmodel.save_pretrained(\"/kaggle/working/blip-flickr30k\")\nprocessor.save_pretrained(\"/kaggle/working/blip-flickr30k\")\nprint(\"Model saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 12. TESTING AND BLEU SCORE\n\nmodel.eval()\nbleu_scores = []\n\nfor idx in range(len(test_dataset)):\n    data = test_dataset[idx]\n    pixel_values = data[\"pixel_values\"].unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        generated_ids = model.generate(pixel_values=pixel_values, max_length=30)\n        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n\n    reference = [test_df.iloc[idx]['caption'].split()]\n    candidate = caption.split()\n    bleu = sentence_bleu(reference, candidate)\n    bleu_scores.append(bleu)\n\nprint(f\"Average BLEU Score on Test Set: {sum(bleu_scores)/len(bleu_scores):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:16:18.222934Z","iopub.execute_input":"2025-08-26T10:16:18.223292Z","iopub.status.idle":"2025-08-26T10:16:18.364935Z","shell.execute_reply.started":"2025-08-26T10:16:18.223263Z","shell.execute_reply":"2025-08-26T10:16:18.363729Z"}},"outputs":[],"execution_count":13}]}